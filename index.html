<!DOCTYPE html>
<html>

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>
    Shaocong (Daniel) Xu
  </title>
  <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <!-- Open Graph -->


  <!-- Bootstrap & MDB -->
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg=="
    crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css"
    integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q=="
    crossorigin="anonymous" />

  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"
    integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog=="
    crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css"
    integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg=="
    crossorigin="anonymous">
  <link rel="stylesheet" type="text/css"
    href="https://fonts.useso.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

  <!-- Code Syntax Highlighting -->
  <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes/github.css">

  <!-- Styles -->

  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/">


  <!-- Dark Mode -->
  <script src="/assets/js/theme.js"></script>
  <script src="/assets/js/dark_mode.js"></script>



  <script src="https://kit.fontawesome.com/d6b2d2c6e3.js" crossorigin="anonymous"></script>




  <style>
    .cv_class {
      color: var(--global-text-color);
      font-family: "Font Awesome 6 Brands";
    }

    .cv_class:hover {
      color: var(--global-theme-color);
      font-family: "Font Awesome 6 Brands";
    }
  </style>
</head>

<body class="fixed-top-nav ">

  <!-- Header -->

  <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
      <div class="container">

        <!-- Social Icons -->

        <div class="navbar-brand social">

          <a class="cv_class"
            href="https://drive.google.com/file/d/1bjz3zk5mcd3QRNqvXgYma-Eb0_sF-llK/view?usp=drive_link">CV</a>

          <a href="mailto:xushaocong@stu.xmu.edu.cn"><i class="fas fa-envelope"></i></a>

          <!-- modified by shaocong xu -->
          <a href="https://scholar.google.com/citations?user=PvYOrK0AAAAJ&hl=zh-CN&oi=ao" title="Google Scholar"
            target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
          <a href="https://github.com/Daniellli" title="GitHub" target="_blank" rel="noopener noreferrer"><i
              class="fab fa-github"></i></a>


          <!-- <a href="https://www.zhihu.com/people/shaocong.xu/" target="_blank" rel="noopener">
          <i class="fab fa-zhihu big-icon" aria-hidden="true"></i>
        </a> -->

          <a href="https://twitter.com/xshocng1" target="_blank" rel="noopener">
            <i class="fab fa-twitter big-icon" aria-hidden="true"></i>
          </a>

          <a href="https://space.bilibili.com/485637351?spm_id_from=333.1007.0.0" target="_blank" rel="noopener">
            <i class="fab fa-bilibili big-icon"></i></a>



        </div>

        <!-- Navbar Toggle -->
        <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
          aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar top-bar"></span>
          <span class="icon-bar middle-bar"></span>
          <span class="icon-bar bottom-bar"></span>
          
        </button>
        <div class="collapse navbar-collapse text-right" id="navbarNav">
          <ul class="navbar-nav ml-auto flex-nowrap">
            <!-- About -->
            <li class="nav-item active">
              <a class="nav-link" href="/">
                about
                <span class="sr-only">(current)</span>
              </a>
            </li>
            <!-- Other pages -->
            <div class="toggle-container">
              <a id="light-toggle">
                <i class="fas fa-moon"></i>
                <i class="fas fa-sun"></i>
              </a>
            </div>
          </ul>

        </div>
      </div>
    </nav>
  </header>


  <!-- Content -->

  <div class="container mt-5">
    <div class="post">

      <header class="post-header">
        <h1 class="post-title">
          Shaocong (Daniel) Xu
        </h1>
        <p class="desc">Xiamen University</p>
      </header>
      <article>
        <div class="profile float-right">
          <!-- 
            prof_pic-chongqing-3.jpg
          -->
        
          <img class="img-fluid z-depth-1 rounded" src="/assets/resized/prof_pic-chongqing-3-654X621.jpg"
            srcset="/assets/resized/prof_pic-chongqing-3-485X460.jpg 480w,/assets/resized/prof_pic-chongqing-3-654X621.jpg 648w">
        </div>
        <div class="clearfix">

          <!-- <p>I am an assistant professor in 
        <a href="https://en.wikipedia.org/wiki/Zhejiang_University" target="_blank" rel="noopener noreferrer">Zhejiang University</a>. 
        Before that, I was a Postdoc in <a href="http://cvlibs.net/" target="_blank" rel="noopener noreferrer">Autonomous Vision Group</a>, 
        a part of the University of TÃ¼bingen and the MPI for Intelligent Systems,
         working with <a href="https://avg.is.tuebingen.mpg.de/person/ageiger" target="_blank" rel="noopener noreferrer">Prof. Andreas Geiger</a>. 
         I received my Ph.D. in Control Science and Engineering from 
         <a href="https://en.wikipedia.org/wiki/Zhejiang_University" target="_blank" rel="noopener noreferrer">Zhejiang University</a> 
         in June 2018 and the B.S. degree from 
         <a href="https://en.wikipedia.org/wiki/Xiamen_University" target="_blank" rel="noopener noreferrer">Xiamen University</a> in 2013.</p> -->
          <p>
            <!-- I am computer vision engineer in  <a href="https://www.aibee.cn/" target="_blank" rel="noopener noreferrer">AIBEE</a>, working closed with
             Dr. <a href="https://www.f-zhou.com/" target="_blank" rel="noopener noreferrer">Feng Zhou</a>. Before that, I obtained my M.S. Degree in computer science from 
             <a href="https://en.wikipedia.org/wiki/Xiamen_University" target="_blank" rel="noopener noreferrer">Xiamen
              University</a> (2021-present), being guided by
            Prof. <a href="https://www.humanplus.xyz/team" target="_blank" rel="noopener noreferrer">Shihui Guo</a>. During that time, I was honor to be a research intern at the DISCOVER lab, <a
              href="https://en.wikipedia.org/wiki/Tsinghua_University" target="_blank"
              rel="noopener noreferrer">Tsinghua University</a>. -->

            I am a computer vision engineer at <a href="https://www.baai.ac.cn/" target="_blank"
              rel="noopener noreferrer">BAAI</a>.
            Before that, I obtained my M.S. degree in computer science from <a
              href="https://en.wikipedia.org/wiki/Xiamen_University" target="_blank" rel="noopener noreferrer">Xiamen
              University</a>,
            under the guidance of Prof. <a href="https://www.humanplus.xyz/team" target="_blank"
              rel="noopener noreferrer">Shihui Guo</a>.
            During my master's program, I was honored to be a research intern at the DISCOVER Lab, <a
              href="https://en.wikipedia.org/wiki/Tsinghua_University" target="_blank"
              rel="noopener noreferrer">Tsinghua University</a>,
            under the guidance of Prof. <a href="https://sites.google.com/view/fromandto" target="_blank"
              rel="noopener noreferrer">Hao Zhao</a>.




            <!-- I received my B.S. degree from <a href="https://en.wikipedia.org/wiki/Xiamen_University_of_Technology"
              target="_blank" rel="noopener noreferrer">Xiamen Univesity of Technology</a> in 2021. -->



          </p>
          <p>My research interests are primarily focused on the field of computer vision, with a particular emphasis
            on areas such as scene perception,
            unsupervised learning, semi-supervised learning, and language-guided scene perception.
          </p>

          <!-- <p><strong style="color: green; background-color: #ffff42">Announcement</strong>: I am actively looking for
            <strong>Ph.D in 2025Fall! </strong></p> -->

          <!-- feel free to contact me via <a href="mailto:yiyi.liao@zju.edu.cn">email</a> -->


        </div>

        <!--!=========================news block ======================-->
        <div class="news">
          <h2>news</h2>
          <div class="table-responsive", style="max-height: 300px; overflow-y: auto;">
            <table class="table table-sm table-borderless" style="width: 100%">
              <colgroup>
                <col span="1" style="width: 15%;">
                <col span="1" style="width: 85%;">
              </colgroup>
              <!-- Put <thead>, <tbody>, and <tr>'s here! -->
              <tbody>

                <!-- <tr>
                  <th scope="row">Feb, 2025</th>
                  <td>
                    Invited talk at 
                    <a href="https://www.bilibili.com/video/BV1fGcneEEwj/?spm_id_from=333.1387.collection.video_card.click&vd_source=3dbee2c7b78b0a77ef3b8f9b2ac6c209" target="_blank" rel="noopener noreferrer"> AutoDrivingTech ("èªå¨é©¾é©¶ä¹å¿" in Chinese).</a>.
                  </td>
                </tr> -->



                <tr>
                  <th scope="row">Jun, 2025</th>
                  <td>
                     <a href="https://houyuanchen111.github.io/lino.github.io/" target="_blank" rel="noopener noreferrer">LiNo</a>, is now available. 
                  </td>
                </tr>





                <tr>
                  <th scope="row">Jun, 2025</th>
                  <td>
                     <a href="https://orangesodahub.github.io/ORV/" target="_blank" rel="noopener noreferrer">ORV</a>, the first world model of BAAI-CWM team, is now available. 
                  </td>
                </tr>


                  <tr>
                  <th scope="row">Jun, 2025</th>
                  <td>
                    <a href="https://bigcileng.github.io/bilateral-driving/" target="_blank" rel="noopener noreferrer">Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian Splatting</a> is now available.
                  </td>
                </tr>



                <tr>
                  <th scope="row">Jan, 2025</th>
                  <td>
                    CoopDETR is accepted to <a href="https://2025.ieee-icra.org/" target="_blank" rel="noopener noreferrer">ICRA2025</a>.
                  </td>
                </tr>



                <tr>
                  <th scope="row">Jan, 2025</th>
                  <td>
                    Invited talk at 
                    <a href="https://www.bilibili.com/video/BV1fGcneEEwj/?spm_id_from=333.1387.collection.video_card.click&vd_source=3dbee2c7b78b0a77ef3b8f9b2ac6c209" target="_blank" rel="noopener noreferrer">AITIME</a>.
                  </td>
                </tr>


                <tr>
                  <th scope="row">Dec, 2024</th>
                  <td>
                    I join <a href="https://www.baai.ac.cn/" target="_blank" rel="noopener noreferrer">BAAI</a>
                    as an algorithm engineer working on controllable world model.   
                  </td>
                </tr>

                <tr>
                  <th scope="row">Dec, 2024</th>
                  <td>
                    <a href="#"
                      target="_blank" rel="noopener noreferrer">LiON</a> is accepted
                    to <a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank"
                      rel="noopener noreferrer">AAAI2025</a>.
                  </td>
                </tr>


                <tr>
                  <th scope="row">Sept, 2024</th>
                  <td>
                    <a href="https://www.sciencedirect.com/science/article/pii/S2772375524001898?via%3Dihub"
                      target="_blank" rel="noopener noreferrer">Intelligent Beehive Monitoring System based on Internet
                      of Things and Colony State Analysis</a> is accepted
                    to <a href="https://www.sciencedirect.com/journal/smart-agricultural-technology" target="_blank"
                      rel="noopener noreferrer">Smart Agricultural Technology (JCR1, IF6.3) </a>.
                  </td>
                </tr>


                <tr>
                  <th scope="row">Jul, 2024</th>
                  <td>
                    I join <a href="https://www.aibee.cn/" target="_blank" rel="noopener noreferrer">AIBEE Inc.</a>
                    as an algorithm engineer working on Uniform Retrieval with <a
                      href="https://scholar.google.com/citations?user=zHpew00AAAAJ&hl=zh-CN" target="_blank"
                      rel="noopener noreferrer">Feng Zhou</a>.


                  </td>
                </tr>



                <tr>
                  <th scope="row">Jun, 2024</th>
                  <td>
                    I graduate as a Master in computer science.
                    <a href="/news/graduation_from_XMU/index.html">
                      photos
                      <!-- <i class="fa fa-camera"></i>
                      <i class="fa fa-video-camera"></i>
                      <i class="fa fa-camera-retro"></i>
                      <i class="fa fa-photo-video"></i> -->
                      <!-- <i class="fa fa-picture-o"></i> -->
                      <i class="fa fa-images"></i>
                    </a>
                  </td>
                </tr>


                <tr>
                  <th scope="row">Feb, 2024</th>
                  <td>
                    <a href="https://arxiv.org/abs/2308.03092" target="_blank" rel="noopener noreferrer">ECT</a> is
                    accepted
                    to <a href="https://www.sciencedirect.com/journal/image-and-vision-computing" target="_blank"
                      rel="noopener noreferrer">IVC (JCR1, IF4.7) </a>.
                  </td>
                </tr>


                <tr>
                  <th scope="row">Sept, 2023</th>
                  <td>
                    <a href="https://aclanthology.org/2023.ccl-4.4/" target="_blank"
                      rel="noopener noreferrer">Foundation Models for Robotics: Best Known Practices</a>, is available.
                  </td>
                </tr>

                <tr>
                  <th scope="row">Jul, 2023</th>
                  <td>
                    <a href="https://int2.cn/" target="_blank" rel="noopener noreferrer">INT2</a> is accepted to
                    ICCV2023.
                  </td>
                </tr>


                <tr>
                  <th scope="row">Jun, 2023</th>
                  <td>
                    We secure the second place in the <a href="https://scene-understanding.com/challenge.html"
                      target="_blank" rel="noopener noreferrer">CVPR 2023 Situation Understanding Challenge</a>.
                    Our technical report, <a
                      href="https://drive.google.com/file/d/1HVc8qM0BEdVhkkOjHnTlYWoZTOy4PQAO/view?usp=drive_link"
                      target="_blank" rel="noopener noreferrer">SLiP: Situated-oriented Localization in Point
                      Clouds</a>, is now available.
                  </td>
                </tr>
              </tbody>

            </table>
          </div>

        </div>
        <!--!=========================================================-->

        <!--!=========================AccomplishÂ­ments ======================-->
        <div class="news">
          <h2>accomplishÂ­ments</h2>
          <div class="table-responsive">
            <table class="table table-sm table-borderless" style="width: 100%">
              <colgroup>
                <col span="1" style="width: 15%;">
                <col span="1" style="width: 85%;">
              </colgroup>
              <!-- Put <thead>, <tbody>, and <tr>'s here! -->
              <tbody>

                <tr>
                  <th scope="row">June 21, 2021</th>
                  <td>
                    I was honored to receive the prestigious National Scholarship, which is highest scholarship awarded
                    by the Ministry of Education of China.
                  </td>
                </tr>

                <!-- <tr>
                    <th scope="row">Jun 24, 2021</th>
                    <td>
                      Our work <a href="https://github.com/fabiotosi92/SMD-Nets" target="_blank"
                        rel="noopener noreferrer">SMD-Nets</a> was featured on the <a
                        href="https://www.rsipvision.com/CVPR2021-Wednesday/6/" target="_blank"
                        rel="noopener noreferrer">CVPR Daily</a> and the <a
                        href="https://www.rsipvision.com/ComputerVisionNews-2021July/30/" target="_blank"
                        rel="noopener noreferrer">BEST OF CVPR of Computer Vision News</a>.
                    </td>
                  </tr> -->

              </tbody>
              <!--
        <tr>
          <td class="date"></td>
        <td class="announcement">
          <a class="all-news" href="https://yiyiliao.github.io/news/">All older news items</a>
        </td>
        </tr>
        -->
            </table>
          </div>

        </div>
        <!--!=========================================================-->




        <div class="publications">
          <h2>selected publications</h2>
          <p>
            <!-- Full publication list can be found on -->
            <!-- <a href="https://scholar.google.com/citations?user=lTBMax0AAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Google Scholar</a>. -->
            <!-- <br> -->
            <sup>*</sup>equal contribution; <sup>â¯</sup>corresponding author.
          </p>
          <h2 class="year">2024</h2>
          <ol class="bibliography">
            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">
                    <img src="/assets/teaser/pad_teaser.gif"
                      alt="Learning Point-wise Abstaining Penalty for Point Cloud Anomaly Detection"
                      style="width: 100%;">
                  </div>

                </div>

                <div id="Schwarz2020NEURIPT" class="col-md-9">
                  <div class="title">LiON: Learning Point-wise Abstaining Penalty for LiDAR Outlier DetectioN Using Diverse Synthetic Data</div>

                  <div class="author">
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    <a href="https://scholar.google.com/citations?user=hmii_L8AAAAJ&hl=zh-CN&oi=sra" target="_blank"
                      rel="noopener noreferrer">Pengfei Li</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Qianpu Sun</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Xinyu Liu</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yang Li</a>,
                    <a href="https://www.humanplus.xyz/team" target="_blank" rel="noopener noreferrer">Shihui Guo</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Zhen Wang</a>,
                    <a href="https://ieeexplore.ieee.org/author/37088230717" target="_blank"
                      rel="noopener noreferrer">Bo Jiang</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Rui Wang</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Kehua Sheng</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Bo Zhang</a>,
                    <a href="https://scholar.google.com/citations?user=5cIodxsAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Li Jiang</a>,
                    <a href="https://sites.google.com/view/fromandto" target="_blank" rel="noopener noreferrer">Hao Zhao</a>, 
                    and <a href="#" target="_blank" rel="noopener noreferrer">Yilun Chen</a>

                  </div>
                  <div class="periodical">
                    <em>In Proc. of the Association for the Advancement of Artificial Intelligence (AAAI) 2025</em>
                  </div>
                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                    <a href="https://arxiv.org/abs/2309.10230" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">PDF</a>
                    <a href="https://github.com/Daniellli/PAD.git" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Code</a>


                    <a href="https://www.youtube.com/watch?v=nvLgVpTZhu8" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Video Demo</a>
                    <a href="https://mp.weixin.qq.com/s/Or67Xr4E_HvOcV_BOKp0Qg" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">blog</a>
                      

                    <a href="https://drive.google.com/file/d/1cBOF-W2nllM12zANxAl0O6DRMz5KCea1/view?usp=drive_link" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">poster</a>
                      

                  </div>

                  <div class="abstract hidden">
                    <p>
                      LiDAR-based semantic scene understanding is an important
                      module in the modern autonomous driving perception stack.
                      However, identifying Out-Of-Distribution points in a
                      LiDAR point cloud is challenging as point clouds lack semantically rich features when compared
                      with RGB images. We
                      revisit this problem from the perspective of selective classification, which introduces a
                      selective function into the standard
                      closed-set classification setup. Our solution is built upon the
                      basic idea of abstaining from choosing any known categories
                      but learns a point-wise abstaining penalty with a marginbased loss. Synthesizing outliers to
                      approximate unlimited
                      OOD samples is also critical to this idea, so we propose a
                      strong synthesis pipeline that generates outliers originated
                      from various factors: unrealistic object categories, sampling
                      patterns and sizes. We demonstrate that learning different abstaining penalties, apart from
                      point-wise penalty, for different
                      types of (synthesized) outliers can further improve the performance. We benchmark our method on
                      SemanticKITTI and
                      nuScenes and achieve state-of-the-art results. Risk-coverage
                      analysis further reveals intrinsic properties of different methods.
                    </p>
                  </div>
                </div>
              </div>
            </li>

            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">
                    <img src="/assets/teaser/coopdetr.gif"
                      alt="CoopDETR: A Unified Cooperative Perception Framework for 3D Detection via Object Query"
                      style="width: 100%;">
                  </div>

                </div>

                <div id="Schwarz2020NEURIPT" class="col-md-9">

                  <div class="title">CoopDETR: A Unified Cooperative Perception Framework for 3D Detection via Object
                    Query</div>


                  <div class="author">
                    <a href="https://scholar.google.com/citations?user=y_vW-jcAAAAJ&hl=zh-CN" target="_blank"
                      rel="noopener noreferrer">Zhe Wang</a>,
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Zhuang Xucai</a>,
                    <a href="https://scholar.google.com.hk/citations?user=LO8GS7sAAAAJ&hl=zh-CN" target="_blank"
                      rel="noopener noreferrer">Tongda Xu</a>,
                    <a href="https://scholar.google.com/citations?user=QOZnsYYAAAAJ&hl=zh-CN" target="_blank"
                      rel="noopener noreferrer">Yan Wang</a>,
                    <a href="https://scholar.google.com/citations?hl=en&user=BzJ_GboAAAAJ&view_op=list_works&sortby=pubdate"
                      target="_blank" rel="noopener noreferrer">Jingjing Liu</a>,
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=XGnsL5MAAAAJ&view_op=list_works&sortby=pubdate"
                      target="_blank" rel="noopener noreferrer">Yilun Chen</a>, and
                    <a href="https://scholar.google.com/citations?user=mDOMfxIAAAAJ&hl=en" target="_blank"
                      rel="noopener noreferrer">Ya-Qin Zhang</a>,
                  </div>
                  <div class="periodical">
                    <em>In Proc. of the IEEE International Conference on Robotics & Automation (ICRA) 2025</em>
                    <!-- 2024 -->
                  </div>
                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                    <a href="https://arxiv.org/abs/2502.19313" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">PDF</a>
                    <!-- <a href="https://github.com/Daniellli/PAD.git" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Code</a> -->


                    <a href="https://www.youtube.com/watch?v=JkhE2jGVV7c" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Video Demo</a>

                  </div>

                  <div class="abstract hidden">
                    <p>
                      Cooperative perception enhances the individual
                      perception capabilities of autonomous vehicles (AVs) by providing a comprehensive view of the
                      environment. However, balancing perception performance and transmission costs remains a
                      significant challenge. Current approaches that transmit regionlevel features across agents are
                      limited in interpretability and
                      demand substantial bandwidth, making them unsuitable for
                      practical applications. In this work, we propose CoopDETR, a
                      novel cooperative perception framework that introduces objectlevel feature cooperation via object
                      query. Our framework
                      consists of two key modules: single-agent query generation,
                      which efficiently encodes raw sensor data into object queries,
                      reducing transmission cost while preserving essential information for detection; and cross-agent
                      query fusion, which includes
                      Spatial Query Matching (SQM) and Object Query Aggregation
                      (OQA) to enable effective interaction between queries. Our
                      experiments on the OPV2V and V2XSet datasets demonstrate
                      that CoopDETR achieves state-of-the-art performance and
                      significantly reduces transmission costs to 1/782 of previous methods
                    </p>
                  </div>
                </div>
              </div>
            </li>

            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">
                    <img src="/assets/teaser/ect_teaser.gif"
                      alt="ECT: Fine-grained Edge Detection with Learned Cause Tokens" style="width: 100%;">
                  </div>

                </div>

                <div id="Schwarz2020NEURIPT" class="col-md-9">
                  <div class="title">ECT: Fine-grained Edge Detection with Learned Cause Tokens</div>

                  <div class="author">
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    <a href="https://github.com/cxx226/cxx226.github.io/blob/master/about.md" target="_blank"
                      rel="noopener noreferrer">
                      Xiaoxue Chen
                    </a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yuhang Zheng</a>,
                    <a href="https://air.tsinghua.edu.cn/info/1046/1199.htm" target="_blank"
                      rel="noopener noreferrer">Guyue Zhou</a>,
                    <a href="https://www.intel.com/content/www/us/en/research/researchers/yurong-chen.html"
                      target="_blank" rel="noopener noreferrer">Yurong Chen</a>,
                    <a href="https://www.cis.pku.edu.cn/info/1177/1379.htm" target="_blank"
                      rel="noopener noreferrer">Hongbin Zha</a>,
                    and <a href="https://sites.google.com/view/fromandto" target="_blank" rel="noopener noreferrer">Hao
                      Zhao</a>
                  </div>
                  <div class="periodical">
                    <!-- <em>In Advances in Neural Information Processing Systems (NeurIPS)</em> -->
                    <em><a href="https://www.sciencedirect.com/science/article/abs/pii/S0262885624000507" target="_blank"
                        rel="noopener noreferrer">Image and Vision Computing (IVC, JCR1, IF4.7)</a></em>
                    2024
                  </div>
                  <div class="links">
                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>

                    <a href="https://authors.elsevier.com/c/1ie6oxnVKF2-x" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">PDF</a>

                    <!-- <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS_supplementary.pdf"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a> -->

                    <!-- <a href="https://autonomousvision.github.io/graf/" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Blog</a> -->

                    <a href="https://github.com/Daniellli/ECT.git" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Code</a>

                    <!-- <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS_poster.pdf"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> -->

                    <!-- <a href="https://www.bilibili.com/video/BV1Lz4y1W7rN/?vd_source=3dbee2c7b78b0a77ef3b8f9b2ac6c209"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video Demo</a> -->
                    <a href="https://www.youtube.com/watch?v=MdtK8SKo0nc" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Video Demo</a>

                  </div>
                  <!-- Hidden abstract block -->
                  <div class="abstract hidden">
                    <p>
                      In this study, we tackle the challenging fine-grained edge detection task, which refers to
                      predicting specific edges caused by reflectance,
                      illumination, normal, and depth changes, respectively. Prior methods exploit multi-scale
                      convolutional networks,
                      which are limited in three aspects: (1) Convolutions are local operators while identifying the
                      cause of edge formation requires looking at far away pixels.
                      (2) Priors specific to edge cause are fixed in prediction heads.
                      (3) Using separate networks for generic and fine-grained edge detection, and the constraint
                      between them may be violated.
                      To address these three issues, we propose a two-stage transformer-based network sequentially
                      predicting generic edges and fine-grained edges,
                      which has a global receptive field thanks to the attention mechanism. The prior knowledge of edge
                      causes is formulated as four learnable cause tokens in
                      a cause-aware decoder design. Furthermore, to encourage the consistency between generic edges and
                      fine-grained edges,
                      an edge aggregation and alignment loss is exploited. We evaluate our method on the public
                      benchmark BSDS-RIND and several newly derived benchmarks,
                      and achieve new state-of-the-art results.
                    </p>
                  </div>
                  <!-- Hidden bibtex block -->
                </div>
              </div>
            </li>


            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">

                    <img src="/assets/teaser/iot-teaser.gif"
                      alt="Intelligent Beehive Monitoring System based on Internet of Things and Colony State Analysis"
                      style="width: 100%;">
                  </div>

                </div>


                <div id="Schwarz2020NEURIPT" class="col-md-9">
                  <div class="title">Intelligent Beehive Monitoring System based on Internet of Things and Colony State
                    Analysis</div>

                  <div class="author">
                    <a href="#" target="_blank" rel="noopener noreferrer">Yiyao Zheng</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Xiaoyan Cao</a>,
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Shihui Guo</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Rencai Huang</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yingjiao Li</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yijie Chen</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Liulin Yang</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Xiaoyu Cao</a>,
                    and <a href="#" target="_blank" rel="noopener noreferrer">Hongting Sun</a>
                  </div>
                  <div class="periodical">
                    <em>Smart Agricultural Technology (JCR1, IF6.3) </em>
                    2024
                  </div>
                  <div class="links">
                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                    <a href="https://www.sciencedirect.com/science/article/pii/S2772375524001898?via%3Dihub"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                    <!-- <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS_supplementary.pdf"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a> -->

                    <!-- <a href="https://github.com/AIR-DISCOVER/INT2.git" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Code</a> -->
                    <a href="https://www.bilibili.com/video/BV1Eo4y1k7BC/?spm_id_from=333.999.0.0&vd_source=3dbee2c7b78b0a77ef3b8f9b2ac6c209"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video
                      Demo</a>
                  </div>
                  <!-- Hidden abstract block -->
                  <div class="abstract hidden">
                    <p>
                      CONTEXT: Bees play a crucial role in terrestrial ecosystems.
                      However, Beekeepers are unable to monitor the state of beehives (bees and environment) all the
                      time,
                      which often results in Colony Collapse Disorder (CCD). Currently,
                      some researchers provided the scheme of intelligent beehive monitoring system equipped
                      with the Internet of Things (IoT). There remain two challenges:
                      Accurately monitor the environmental status around the hive and accurately track and monitor bees
                      in real time.
                      <br>

                      OBJECTIVE: With the development of the IoT and computer vision algorithms,
                      we hope to provide an automated and efficient system to meet the above challenges.
                      In this paper, we propose a hive monitoring system and build a visualization module
                      in the cloud to monitor the activity of bee colonies and the environmental dynamic changes.

                      <br>

                      METHODS: We proposed a multi-bee tracking algorithm to solve
                      the problem of monitoring bees at the door of the hive.
                      We construct a dataset containing various complex scenes, named BEE22,
                      for training and testing the performance of our algorithm.
                      We design a bee counting rule, based on results of multi-bee tracking algorithm,
                      to reasonably count the bees entering or leaving the beehive.
                      We have deployed multiple sensors around (center, margin, door, and environment)
                      the hive to accurately reflect the changes in the environment around the hive.


                      <br>
                      RESULTS AND CONCLUSIONS: Experimental results demonstrate the effectiveness and superiority
                      of our system. In particular, the tracking performance of the multi-bee tracking algorithm
                      reaches 83.5% Â± 0.7% accuracy (MOTA) and 77.3% Â± 0.2% MOTP, speeds up to 16 frames per second,
                      compared with other algorithms, MOTA and IDF1 are improved by 24.4% and 50.4% respectively.
                      Moreover, our counting algorithm also achieved excellent results,
                      with root mean square error (RMSE) of 0.17 Â± 0.01, 1.55 Â± 0.05, and 1.25 Â± 0.06 in counting
                      the number of bees entering, leaving, and the current scene in an episode, respectively.
                      After that, the system will be deployed and monitored for a long time in the actual scenario,
                      it was found that the activity of bees decreased significantly under heavy rainfall conditions.
                      Additionally, the activity of the bee colony will also increase accordingly,
                      when the amplitude is 500dB to 2000dB, the temperature of the center of the beehive is 25Â°C to
                      37Â°C,
                      and the humidity is 48% to 67%.

                      <br>

                      SIGNIFICANCE: Our system can provide valuable information for bee farmers to make control
                      decisions on hives.
                    </p>
                  </div>
                  <!-- Hidden bibtex block -->
                </div>
              </div>
            </li>
          </ol>
          <h2 class="year">2023</h2>

          <ol class="bibliography">

            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">

                    <img src="/assets/teaser/int2_teaser.gif"
                      alt="INT2: Interactive Trajectory Prediction at Intersections" style="width: 100%;">
                  </div>

                </div>

                <div id="Schwarz2020NEURIPT" class="col-md-9">
                  <div class="title">INT2: Interactive Trajectory Prediction at Intersections</div>

                  <div class="author">
                    <a href="#" target="_blank" rel="noopener noreferrer">Zhijie Yan</a>,
                    <a href="https://scholar.google.com/citations?user=hmii_L8AAAAJ&hl=zh-CN&oi=sra" target="_blank"
                      rel="noopener noreferrer">Pengfei Li</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Zheng Fu</a>,
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yongliang Shi</a>,
                    <a href="https://github.com/cxx226/cxx226.github.io/blob/master/about.md" target="_blank"
                      rel="noopener noreferrer">Xiaoxue Chen</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yuhang Zheng</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yang Li</a>,
                    <a href="https://scholar.google.com/citations?user=NAt3vgcAAAAJ&hl=zh-CN&oi=sra" target="_blank"
                      rel="noopener noreferrer">Tianyu Liu</a>,

                    <a href="#" target="_blank" rel="noopener noreferrer">Chuxuan Li</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Nairui Luo</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Xu Gao</a>,
                    <a href="https://air.tsinghua.edu.cn/en/info/1046/1621.htm" target="_blank"
                      rel="noopener noreferrer">Yilun Chen</a>,

                    <a href="http://wangzuoxu.cn/" target="_blank" rel="noopener noreferrer">Zuoxu Wang</a>,
                    <a href="https://scholar.google.com/citations?user=KlHuj2QAAAAJ&hl=zh-CN&oi=ao" target="_blank"
                      rel="noopener noreferrer">Yifeng Shi </a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Pengfei Huang</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Zhengxiao Han</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Jirui Yuan</a>,
                    <a href="https://scholar.google.com/citations?user=AktmI14AAAAJ&hl=zh-CN&oi=ao" target="_blank"
                      rel="noopener noreferrer">Jiangtao Gong</a>,
                    <a href="https://air.tsinghua.edu.cn/info/1046/1199.htm" target="_blank"
                      rel="noopener noreferrer">Guyue Zhou</a>,
                    <a href="https://hangzhaomit.github.io/" target="_blank" rel="noopener noreferrer">Hang Zhao</a> and
                    <a href="https://sites.google.com/view/fromandto" target="_blank" rel="noopener noreferrer">Hao
                      Zhao</a>
                  </div>
                  <div class="periodical">
                    <em>In Proc. of the IEEE International Conf. on Computer Vision (ICCV)</em>
                    2023
                  </div>
                  <div class="links">
                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>

                    <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yan_INT2_Interactive_Trajectory_Prediction_at_Intersections_ICCV_2023_paper.html"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                    <!-- <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS_supplementary.pdf"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a> -->

                    <a href="https://github.com/AIR-DISCOVER/INT2.git" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Code</a>
                    <a href="https://int2.cn/" class="btn btn-sm z-depth-0" role="button" target="_blank"
                      rel="noopener noreferrer">Project Page</a>
                    <a href="https://int2.cn/download" class="btn btn-sm z-depth-0" role="button" target="_blank"
                      rel="noopener noreferrer">Dataset</a>
                    <a href="https://www.youtube.com/watch?v=KNkuakDvgVc" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Video Demo</a>
                  </div>
                  <!-- Hidden abstract block -->
                  <div class="abstract hidden">
                    <p>
                      Motion forecasting is an important component in autonomous driving systems.
                      One of the most challenging problems in motion forecasting is interactive trajectory prediction,
                      whose goal is to jointly forecasts the future trajectories of interacting agents.
                      To this end, we present a large-scale interactive trajectory prediction dataset named INT2 for
                      INTeractive trajectory prediction at INTersections.
                      INT2 includes 612,000 scenes, each lasting 1 minute, containing up to 10,200 hours of data. The
                      agent trajectories are auto-labeled by a high-performance offline temporal detection and fusion
                      algorithm,
                      whose quality is further inspected by human judges. Vectorized semantic maps and traffic light
                      information are also provided. Additionally, the dataset poses an interesting domain mismatch
                      challenge.
                      For each intersection, we treat rush-hour and non-rush-hour segments as different domains.
                      We benchmark the best open-sourced interactive trajectory prediction method on INT2 and Waymo Open
                      Motion, under in-domain and cross-domain settings. The dataset, code and models will be made
                      public.
                    </p>
                  </div>
                  <!-- Hidden bibtex block -->
                </div>
              </div>
            </li>

          <!-- </ol>
        </div>

        <div class="publications">
          <h2>miscellaneous</h2>
          <p>
            <sup>*</sup>equal contribution; <sup>â¯</sup>corresponding author.
          </p>
          <h2 class="year">2023</h2>
          <ol class="bibliography"> -->
            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">
                    <img src="/assets/teaser/ccl-paper-teaser.png"
                      alt="Foundation Models for Robotics: Best Known Practices" style="width: 100%;">
                  </div>

                </div>

                <div id="Schwarz2020NEURIPT" class="col-md-9">

                  <div class="title">Foundation Models for Robotics: Best Known Practices</div>


                  <div class="author">
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    and <a href="https://sites.google.com/view/fromandto" target="_blank" rel="noopener noreferrer">Hao
                      Zhao</a>
                  </div>
                  <div class="periodical">
                    <em>Proceedings of the 22nd Chinese National Conference on Computational Linguistics (Volume 4:
                      Tutorial Abstracts)</em>
                    2023
                  </div>
                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                    <a href="https://aclanthology.org/2023.ccl-4.4/" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">PDF</a>
                  </div>

                  <div class="abstract hidden">
                    <p>
                      Artificial general intelligence (AGI) used to be a sci-fi word but recently
                      the surprising general-ization capability of foundation models have triggered
                      a lot of attention to AGI, in both academiaand industry. Large language models
                      can now answer questions or chat with human beings,using fluent sentences and clear
                      reasoning. Diffusion models can now draw pictures of unprece-dented photo-realism,
                      according to human commands and controls. Researchers have also madesubstantial efforts
                      to explore new possibilities for robotics applications with the help of founda-tion models.
                      Since this interdisciplinary field is still under fast development,
                      there is no clearmethodological conclusions for now. In this tutorial,
                      I will briefly go through best known prac-tices that have shown transformative
                      capabilities in several sub-fields. Specifically, there are fiverepresentative paradigms:
                      (1) Using foundation models to allow human-friendly human-car in-teraction;
                      (2) Using foundation models to equip robots the capabilities of understanding vaguehuman needs;
                      (3) Using foundation models to break down complex tasks into achievable sub-tasks;
                      (4) Using foundation models to composite skill primitives so that reinforcement
                      learningcan work with sparse rewards; (5) Using foundation models to bridge languge
                      commands andlow-level control dynamics. I hope these best known practices to inspire NLP
                      researchers.
                    </p>
                  </div>
                </div>
              </div>
            </li>



            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">
                    <img src="/assets/teaser/slip_teaser.png" alt="SLiP: Situated-oriented Localization in Point Clouds"
                      style="width: 100%;">
                  </div>

                </div>

                <div id="Schwarz2020NEURIPT" class="col-md-9">

                  <div class="title">SLiP: Situated-oriented Localization in Point Clouds</div>


                  <div class="author">
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    <a href="https://github.com/cxx226/cxx226.github.io/blob/master/about.md" target="_blank"
                      rel="noopener noreferrer"> Xiaoxue Chen</a>,
                    <a href="https://sites.google.com/view/fromandto" target="_blank" rel="noopener noreferrer">Hao
                      Zhao</a>,
                    and <a href="https://air.tsinghua.edu.cn/info/1046/1199.htm" target="_blank"
                      rel="noopener noreferrer">Guyue Zhou</a>
                  </div>
                  <div class="periodical">
                    <em>Technical Report</em>
                    2023
                  </div>
                  <div class="links">
                    <!-- <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> -->
                    <a href="https://drive.google.com/file/d/1HVc8qM0BEdVhkkOjHnTlYWoZTOy4PQAO/view?usp=drive_link"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                  </div>

                  <!-- <div class="abstract hidden">
                    <p>
                      Artificial general intelligence (AGI) used to be a sci-fi word but recently 
                      the surprising general-ization capability of foundation models have triggered 
                      a lot of attention to AGI, in both academiaand industry. Large language models 
                      can now answer questions or chat with human beings,using fluent sentences and clear 
                      reasoning. Diffusion models can now draw pictures of unprece-dented photo-realism, 
                      according to human commands and controls. Researchers have also madesubstantial efforts 
                      to explore new possibilities for robotics applications with the help of founda-tion models. 
                      Since this interdisciplinary field is still under fast development, 
                      there is no clearmethodological conclusions for now. In this tutorial, 
                      I will briefly go through best known prac-tices that have shown transformative 
                      capabilities in several sub-fields. Specifically, there are fiverepresentative paradigms: 
                      (1) Using foundation models to allow human-friendly human-car in-teraction; 
                      (2) Using foundation models to equip robots the capabilities of understanding vaguehuman needs; 
                      (3) Using foundation models to break down complex tasks into achievable sub-tasks; 
                      (4) Using foundation models to composite skill primitives so that reinforcement 
                      learningcan work with sparse rewards; (5) Using foundation models to bridge languge 
                      commands andlow-level control dynamics. I hope these best known practices to inspire NLP researchers.
                    </p>
                  </div> -->
                </div>
              </div>
            </li>

          </ol>
        </div>
        <!--!=========================================================-->

      </article>
    </div>
  </div>

  <!-- Footer -->






  <footer class="fixed-bottom">
    <div class="container mt-0">
      Â© Copyright 2023 Shaocong Xu.
      Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a
        href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme.
      Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.
      Last updated: December 07, 2022.
    </div>
  </footer>



</body>

<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"
  integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg=="
  crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js"
  integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A=="
  crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"
  integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ=="
  crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js"
  integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw=="
  crossorigin="anonymous"></script>


<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>





<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"
  integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script"
  src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>




<script type="text/javascript"
  src="//rf.revolvermaps.com/0/0/8.js?i=5ukwd0bfk23&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33"
  async="async"></script>

<!-- counter  -->
<!-- <a href="https://info.flagcounter.com/o4lI"><img
    src="https://s11.flagcounter.com/count/o4lI/bg_FFFFFF/txt_000000/border_CCCCCC/columns_1/maxflags_12/viewers_3/labels_1/pageviews_1/flags_0/percent_0/"
    alt="Flag Counter" border="0"></a> -->
    
<!-- new counter  -->
<a href="https://info.flagcounter.com/WncC"><img src="https://s01.flagcounter.com/mini/WncC/bg_EECCFF/txt_000000/border_CCCCCC/flags_0/" alt="Flag Counter" border="0"></a>




</html>