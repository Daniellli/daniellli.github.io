<!DOCTYPE html>
<html>

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>
    Shaocong (Daniel) Xu
  </title>
  <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <!-- Open Graph -->


  <!-- Bootstrap & MDB -->
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg=="
    crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css"
    integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q=="
    crossorigin="anonymous" />

  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"
    integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog=="
    crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css"
    integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg=="
    crossorigin="anonymous">
  <link rel="stylesheet" type="text/css"
    href="https://fonts.useso.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

  <!-- Code Syntax Highlighting -->
  <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes/github.css">

  <!-- Styles -->

  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/">


  <!-- Dark Mode -->
  <script src="/assets/js/theme.js"></script>
  <script src="/assets/js/dark_mode.js"></script>



  <script src="https://kit.fontawesome.com/d6b2d2c6e3.js" crossorigin="anonymous"></script>




  <style>
    .cv_class {
      color: var(--global-text-color);
      font-family: "Font Awesome 6 Brands";
    }

    .cv_class:hover {
      color: var(--global-theme-color);
      font-family: "Font Awesome 6 Brands";
    }
  </style>
</head>

<body class="fixed-top-nav ">

  <!-- Header -->

  <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
      <div class="container">

        <!-- Social Icons -->

        <div class="navbar-brand social">

          <a class="cv_class"
            href="https://drive.google.com/file/d/1DuwPOKLUohm-L29e7SXxJhanFZY986yq/view?usp=sharing">CV</a>

          <a href="mailto:xushaocong@stu.xmu.edu.cn"><i class="fas fa-envelope"></i></a>

          <!-- modified by shaocong xu -->
          <a href="https://scholar.google.com/citations?user=PvYOrK0AAAAJ&hl=zh-CN&oi=ao" title="Google Scholar"
            target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
          <a href="https://github.com/Daniellli" title="GitHub" target="_blank" rel="noopener noreferrer"><i
              class="fab fa-github"></i></a>


          <!-- <a href="https://www.zhihu.com/people/shaocong.xu/" target="_blank" rel="noopener">
          <i class="fab fa-zhihu big-icon" aria-hidden="true"></i>
        </a> -->

          <a href="https://twitter.com/xshocng1" target="_blank" rel="noopener">
            <i class="fab fa-twitter big-icon" aria-hidden="true"></i>
          </a>

          <a href="https://space.bilibili.com/485637351?spm_id_from=333.1007.0.0" target="_blank" rel="noopener">
            <i class="fab fa-bilibili big-icon"></i></a>



        </div>

        <!-- Navbar Toggle -->
        <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
          aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar top-bar"></span>
          <span class="icon-bar middle-bar"></span>
          <span class="icon-bar bottom-bar"></span>
          
        </button>
        <div class="collapse navbar-collapse text-right" id="navbarNav">
          <ul class="navbar-nav ml-auto flex-nowrap">
            <!-- About -->
            <li class="nav-item active">
              <a class="nav-link" href="/">
                about
                <span class="sr-only">(current)</span>
              </a>
            </li>
            <!-- Other pages -->
            <div class="toggle-container">
              <a id="light-toggle">
                <i class="fas fa-moon"></i>
                <i class="fas fa-sun"></i>
              </a>
            </div>
          </ul>

        </div>
      </div>
    </nav>
  </header>


  <!-- Content -->

  <div class="container mt-5">
    <div class="post">

      <header class="post-header">
        <h1 class="post-title">
          Shaocong (Daniel) Xu
        </h1>
        <p class="desc">Xiamen University</p>
      </header>
      <article>
        <div class="profile float-right">
          <!-- 
            prof_pic-chongqing-3.jpg
          -->
        
          <img class="img-fluid z-depth-1 rounded" src="/assets/resized/prof_pic-chongqing-3-654X621.jpg"
            srcset="/assets/resized/prof_pic-chongqing-3-485X460.jpg 480w,/assets/resized/prof_pic-chongqing-3-654X621.jpg 648w">
        </div>
        <div class="clearfix">

          <!-- <p>I am an assistant professor in 
        <a href="https://en.wikipedia.org/wiki/Zhejiang_University" target="_blank" rel="noopener noreferrer">Zhejiang University</a>. 
        Before that, I was a Postdoc in <a href="http://cvlibs.net/" target="_blank" rel="noopener noreferrer">Autonomous Vision Group</a>, 
        a part of the University of Tübingen and the MPI for Intelligent Systems,
         working with <a href="https://avg.is.tuebingen.mpg.de/person/ageiger" target="_blank" rel="noopener noreferrer">Prof. Andreas Geiger</a>. 
         I received my Ph.D. in Control Science and Engineering from 
         <a href="https://en.wikipedia.org/wiki/Zhejiang_University" target="_blank" rel="noopener noreferrer">Zhejiang University</a> 
         in June 2018 and the B.S. degree from 
         <a href="https://en.wikipedia.org/wiki/Xiamen_University" target="_blank" rel="noopener noreferrer">Xiamen University</a> in 2013.</p> -->
          <p>
            <!-- I am computer vision engineer in  <a href="https://www.aibee.cn/" target="_blank" rel="noopener noreferrer">AIBEE</a>, working closed with
             Dr. <a href="https://www.f-zhou.com/" target="_blank" rel="noopener noreferrer">Feng Zhou</a>. Before that, I obtained my M.S. Degree in computer science from 
             <a href="https://en.wikipedia.org/wiki/Xiamen_University" target="_blank" rel="noopener noreferrer">Xiamen
              University</a> (2021-present), being guided by
            Prof. <a href="https://www.humanplus.xyz/team" target="_blank" rel="noopener noreferrer">Shihui Guo</a>. During that time, I was honor to be a research intern at the DISCOVER lab, <a
              href="https://en.wikipedia.org/wiki/Tsinghua_University" target="_blank"
              rel="noopener noreferrer">Tsinghua University</a>. -->

            I am a computer vision researcher at <a href="https://www.baai.ac.cn/" target="_blank"
              rel="noopener noreferrer">BAAI</a>.
            Before that, I obtained my M.S. degree in computer science from <a
              href="https://en.wikipedia.org/wiki/Xiamen_University" target="_blank" rel="noopener noreferrer">Xiamen
              University</a>,
            under the guidance of Prof. <a href="https://www.humanplus.xyz/team" target="_blank"
              rel="noopener noreferrer">Shihui Guo</a>.
            During my master's program, I was honored to be a research intern at the DISCOVER Lab, <a
              href="https://en.wikipedia.org/wiki/Tsinghua_University" target="_blank"
              rel="noopener noreferrer">Tsinghua University</a>,
            under the guidance of Prof. <a href="https://sites.google.com/view/fromandto" target="_blank"
              rel="noopener noreferrer">Hao Zhao</a>.




            <!-- I received my B.S. degree from <a href="https://en.wikipedia.org/wiki/Xiamen_University_of_Technology"
              target="_blank" rel="noopener noreferrer">Xiamen Univesity of Technology</a> in 2021. -->



          </p>
          <p>Current research interest is in controllable generation, including 3D generation and video generation, 
            with a focus on the realism and resolution of generated 3D assets and the potential of video generation models.
          </p>

          <!-- <p><strong style="color: green; background-color: #ffff42">Announcement</strong>: I am actively looking for
            <strong>Ph.D in 2025Fall! </strong></p> -->

          <!-- feel free to contact me via <a href="mailto:yiyi.liao@zju.edu.cn">email</a> -->


        </div>

        <!--!=========================news block ======================-->
        <div class="news">
          <h2>news</h2>
          <div class="table-responsive", style="max-height: 300px; overflow-y: auto;">
            <table class="table table-sm table-borderless" style="width: 100%">
              <colgroup>
                <col span="1" style="width: 15%;">
                <col span="1" style="width: 85%;">
              </colgroup>
              <!-- Put <thead>, <tbody>, and <tr>'s here! -->
              <tbody>

                <!-- <tr>
                  <th scope="row">Feb, 2025</th>
                  <td>
                    Invited talk at 
                    <a href="https://www.bilibili.com/video/BV1fGcneEEwj/?spm_id_from=333.1387.collection.video_card.click&vd_source=3dbee2c7b78b0a77ef3b8f9b2ac6c209" target="_blank" rel="noopener noreferrer"> AutoDrivingTech ("自动驾驶之心" in Chinese).</a>.
                  </td>
                </tr> -->


                <tr>
                  <th scope="row">Nov, 2025</th>
                  <td>
                    <span style="display: inline-block; background: #ff0; color: #d00; font-size: 10px; font-weight: bold; border-radius: 3px; padding: 1px 4px; margin-right: 6px; vertical-align: super;">NEW</span>
                    <a href="https://lightx-ai.github.io/" target="_blank" rel="noopener noreferrer">LightX</a>   is available.                    
                  </td>
                </tr>




                <tr>
                  <th scope="row">Nov, 2025</th>
                  <td>
                    <span style="display: inline-block; background: #ff0; color: #d00; font-size: 10px; font-weight: bold; border-radius: 3px; padding: 1px 4px; margin-right: 6px; vertical-align: super;">NEW</span>
                    <a href="https://arxiv.org/abs/2511.18600" target="_blank" rel="noopener noreferrer">NeAR</a>   is available.                    
                  </td>
                </tr>



                <tr>
                  <th scope="row">Nov, 2025</th>
                  <td>
                    <span style="display: inline-block; background: #ff0; color: #d00; font-size: 10px; font-weight: bold; border-radius: 3px; padding: 1px 4px; margin-right: 6px; vertical-align: super;">NEW</span>
                     GRADRobot is accepted to 3DV2026.
                  </td>
                </tr>




                <tr>
                  <th scope="row">Aug, 2025</th>
                  <td>
                    <span style="display: inline-block; background: #ff0; color: #d00; font-size: 10px; font-weight: bold; border-radius: 3px; padding: 1px 4px; margin-right: 6px; vertical-align: super;">NEW</span>
                     <a href="https://bigcileng.github.io/bilateral-driving/" target="_blank" rel="noopener noreferrer">BilateralDriving</a> is accepted to NeurIPS2025.
                  </td>
                </tr>




                <tr>
                  <th scope="row">Aug, 2025</th>
                  <td>
                     <span style="display: inline-block; background: #ff0; color: #d00; font-size: 10px; font-weight: bold; border-radius: 3px; padding: 1px 4px; margin-right: 6px; vertical-align: super;">NEW</span>
                     <a href="https://gzwsama.github.io/OnePoseviaGen.github.io/" target="_blank" rel="noopener noreferrer">
                       OnePoseviaGen
                     </a>

                     is accepted to 
                     <a href="https://www.corl.org/home" target="_blank" rel="noopener noreferrer">CoRL2025</a> as <strong style="color: red;">Oral</strong> with 
                     a score of <strong style="color: orange;">Strong Acceptance &times; 3</strong>, <strong style="color: green;">Weak Acceptance &times; 1</strong>. 
                     <!-- <span style="color: green;">Congratulations!</span> -->
                  </td>
                </tr>



                <tr>
                  <th scope="row">Aug, 2025</th>
                  <td>
                    <span style="display: inline-block; background: #ff0; color: #d00; font-size: 10px; font-weight: bold; border-radius: 3px; padding: 1px 4px; margin-right: 6px; vertical-align: super;">NEW</span>
                     <a href="https://gzwsama.github.io/OnePoseviaGen.github.io/" target="_blank" rel="noopener noreferrer">OnePoseviaGen</a> is now available                     
                     <a href="https://github.com/gzwsama/OnePoseviaGen" target="_blank" rel="noopener noreferrer" style="margin-left: 6px; font-size: 12px;">
                       <img src="https://img.shields.io/github/stars/gzwsama/OnePoseviaGen?style=social" alt="GitHub stars" style="vertical-align: text-bottom; height: 1em;">
                     </a>. 
                  </td>
                </tr>




                <tr>
                  <th scope="row">Jun, 2025</th>
                  <td>
                    <span style="display: inline-block; background: #ff0; color: #d00; font-size: 10px; font-weight: bold; border-radius: 3px; padding: 1px 4px; margin-right: 6px; vertical-align: super;">NEW</span>
                     <a href="https://houyuanchen111.github.io/lino.github.io/" target="_blank" rel="noopener noreferrer">LiNo</a>, is now available
                     
                     <a href="https://github.com/houyuanchen111/LINO_UniPS" target="_blank" rel="noopener noreferrer" style="margin-left: 6px; font-size: 12px;">
                      <img src="https://img.shields.io/github/stars/houyuanchen111/LINO_UniPS?style=social" alt="GitHub stars" style="vertical-align: text-bottom; height: 1em;">
                    </a>. 
                  </td>
                </tr>





                <tr>
                  <th scope="row">Jun, 2025</th>
                  <td>
                    <span style="display: inline-block; background: #ff0; color: #d00; font-size: 10px; font-weight: bold; border-radius: 3px; padding: 1px 4px; margin-right: 6px; vertical-align: super;">NEW</span>
                     <a href="https://orangesodahub.github.io/ORV/" target="_blank" rel="noopener noreferrer">ORV</a>, the first world model of BAAI-CWM team, is now available
                  <a href="https://github.com/OrangeSodahub/ORV" target="_blank" rel="noopener noreferrer" style="margin-left: 6px; font-size: 12px;">
                    <img src="https://img.shields.io/github/stars/OrangeSodahub/ORV?style=social" alt="GitHub stars" style="vertical-align: text-bottom; height: 1em;">. 
                  </a>
                  </td>
                </tr>


                  <tr>
                  <th scope="row">Jun, 2025</th>
                  <td>
                    <span style="display: inline-block; background: #ff0; color: #d00; font-size: 10px; font-weight: bold; border-radius: 3px; padding: 1px 4px; margin-right: 6px; vertical-align: super;">NEW</span>
                    <a href="https://bigcileng.github.io/bilateral-driving/" target="_blank" rel="noopener noreferrer">BilateralDriving</a> is now available
                    
                    <a href="https://github.com/BigCiLeng/bilateral-driving" target="_blank" rel="noopener noreferrer" style="margin-left: 6px; font-size: 12px;">
                      <img src="https://img.shields.io/github/stars/BigCiLeng/bilateral-driving?style=social" alt="GitHub stars" style="vertical-align: text-bottom; height: 1em;">
                    </a>
                    .
                  </td>
                </tr>



                <tr>
                  <th scope="row">Jan, 2025</th>
                  <td>
                    CoopDETR is accepted to <a href="https://2025.ieee-icra.org/" target="_blank" rel="noopener noreferrer">ICRA2025</a>.
                  </td>
                </tr>



                <tr>
                  <th scope="row">Jan, 2025</th>
                  <td>
                    Invited talk at 
                    <a href="https://www.bilibili.com/video/BV1fGcneEEwj/?spm_id_from=333.1387.collection.video_card.click&vd_source=3dbee2c7b78b0a77ef3b8f9b2ac6c209" target="_blank" rel="noopener noreferrer">AITIME</a>.
                  </td>
                </tr>


                <tr>
                  <th scope="row">Dec, 2024</th>
                  <td>
                    I join <a href="https://www.baai.ac.cn/" target="_blank" rel="noopener noreferrer">BAAI</a>
                    as an algorithm engineer working on controllable world model.   
                  </td>
                </tr>

                <tr>
                  <th scope="row">Dec, 2024</th>
                  <td>
                    <a href="#"
                      target="_blank" rel="noopener noreferrer">LiON</a> is accepted
                    to <a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank"
                      rel="noopener noreferrer">AAAI2025</a>.
                  </td>
                </tr>


                <tr>
                  <th scope="row">Sept, 2024</th>
                  <td>
                    <a href="https://www.sciencedirect.com/science/article/pii/S2772375524001898?via%3Dihub"
                      target="_blank" rel="noopener noreferrer">Intelligent Beehive Monitoring System based on Internet
                      of Things and Colony State Analysis</a> is accepted
                    to <a href="https://www.sciencedirect.com/journal/smart-agricultural-technology" target="_blank"
                      rel="noopener noreferrer">Smart Agricultural Technology (JCR1, IF6.3) </a>.
                  </td>
                </tr>


                <tr>
                  <th scope="row">Jul, 2024</th>
                  <td>
                    I join <a href="https://www.aibee.cn/" target="_blank" rel="noopener noreferrer">AIBEE Inc.</a>
                    as an algorithm engineer working on Uniform Retrieval with <a
                      href="https://scholar.google.com/citations?user=zHpew00AAAAJ&hl=zh-CN" target="_blank"
                      rel="noopener noreferrer">Feng Zhou</a>.


                  </td>
                </tr>



                <tr>
                  <th scope="row">Jun, 2024</th>
                  <td>
                    I graduate as a Master in computer science.
                    <a href="/news/graduation_from_XMU/index.html">
                      photos
                      <!-- <i class="fa fa-camera"></i>
                      <i class="fa fa-video-camera"></i>
                      <i class="fa fa-camera-retro"></i>
                      <i class="fa fa-photo-video"></i> -->
                      <!-- <i class="fa fa-picture-o"></i> -->
                      <i class="fa fa-images"></i>
                    </a>
                  </td>
                </tr>


                <tr>
                  <th scope="row">Feb, 2024</th>
                  <td>
                    <a href="https://arxiv.org/abs/2308.03092" target="_blank" rel="noopener noreferrer">ECT</a> is
                    accepted
                    to <a href="https://www.sciencedirect.com/journal/image-and-vision-computing" target="_blank"
                      rel="noopener noreferrer">IVC (JCR1, IF4.7) </a>.
                  </td>
                </tr>


                <tr>
                  <th scope="row">Sept, 2023</th>
                  <td>
                    <a href="https://aclanthology.org/2023.ccl-4.4/" target="_blank"
                      rel="noopener noreferrer">Foundation Models for Robotics: Best Known Practices</a>, is available.
                  </td>
                </tr>

                <tr>
                  <th scope="row">Jul, 2023</th>
                  <td>
                    <a href="https://int2.cn/" target="_blank" rel="noopener noreferrer">INT2</a> is accepted to
                    ICCV2023.
                  </td>
                </tr>


                <tr>
                  <th scope="row">Jun, 2023</th>
                  <td>
                    We secure the second place in the <a href="https://scene-understanding.com/challenge.html"
                      target="_blank" rel="noopener noreferrer">CVPR 2023 Situation Understanding Challenge</a>.
                    Our technical report, <a
                      href="https://drive.google.com/file/d/1HVc8qM0BEdVhkkOjHnTlYWoZTOy4PQAO/view?usp=drive_link"
                      target="_blank" rel="noopener noreferrer">SLiP: Situated-oriented Localization in Point
                      Clouds</a>, is now available.
                  </td>
                </tr>
              </tbody>

            </table>
          </div>

        </div>
        <!--!=========================================================-->

        <!--!=========================Accomplish­ments ======================-->
        <div class="news">
          <h2>accomplish­ments</h2>
          <div class="table-responsive">
            <table class="table table-sm table-borderless" style="width: 100%">
              <colgroup>
                <col span="1" style="width: 15%;">
                <col span="1" style="width: 85%;">
              </colgroup>
              <!-- Put <thead>, <tbody>, and <tr>'s here! -->
              <tbody>

                <tr>
                  <th scope="row">June 21, 2021</th>
                  <td>
                    I was honored to receive the prestigious National Scholarship, which is highest scholarship awarded
                    by the Ministry of Education of China.
                  </td>
                </tr>

                <!-- <tr>
                    <th scope="row">Jun 24, 2021</th>
                    <td>
                      Our work <a href="https://github.com/fabiotosi92/SMD-Nets" target="_blank"
                        rel="noopener noreferrer">SMD-Nets</a> was featured on the <a
                        href="https://www.rsipvision.com/CVPR2021-Wednesday/6/" target="_blank"
                        rel="noopener noreferrer">CVPR Daily</a> and the <a
                        href="https://www.rsipvision.com/ComputerVisionNews-2021July/30/" target="_blank"
                        rel="noopener noreferrer">BEST OF CVPR of Computer Vision News</a>.
                    </td>
                  </tr> -->

              </tbody>
              <!--
        <tr>
          <td class="date"></td>
        <td class="announcement">
          <a class="all-news" href="https://yiyiliao.github.io/news/">All older news items</a>
        </td>
        </tr>
        -->
            </table>
          </div>

        </div>
        <!--!=========================================================-->

        

        <div class="publications">
          <p>
            <!-- Full publication list can be found on -->
            <!-- <a href="https://scholar.google.com/citations?user=lTBMax0AAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Google Scholar</a>. -->
            <!-- <br> -->
            <sup>*</sup>equal contribution; <sup>♯</sup>corresponding author.
          </p>

          <!-- <h2>Newest  Papers</h2>
          <ol class="bibliography">

          </ol> -->

          <h2>selected publications</h2>
          <h2 class="year">2025</h2>
          <ol class="bibliography">
            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">
                    <img src="/assets/teaser/pad_teaser.gif"
                      alt="Learning Point-wise Abstaining Penalty for Point Cloud Anomaly Detection"
                      style="width: 100%;">
                  </div>

                </div>

                <div id="Schwarz2020NEURIPT" class="col-md-9">
                  <div class="title">LiON: Learning Point-wise Abstaining Penalty for LiDAR Outlier DetectioN Using Diverse Synthetic Data</div>

                  <div class="author">
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    <a href="https://scholar.google.com/citations?user=hmii_L8AAAAJ&hl=zh-CN&oi=sra" target="_blank"
                      rel="noopener noreferrer">Pengfei Li</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Qianpu Sun</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Xinyu Liu</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yang Li</a>,
                    <a href="https://www.humanplus.xyz/team" target="_blank" rel="noopener noreferrer">Shihui Guo</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Zhen Wang</a>,
                    <a href="https://ieeexplore.ieee.org/author/37088230717" target="_blank"
                      rel="noopener noreferrer">Bo Jiang</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Rui Wang</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Kehua Sheng</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Bo Zhang</a>,
                    <a href="https://scholar.google.com/citations?user=5cIodxsAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Li Jiang</a>,
                    <a href="https://sites.google.com/view/fromandto" target="_blank" rel="noopener noreferrer">Hao Zhao</a>, 
                    and <a href="#" target="_blank" rel="noopener noreferrer">Yilun Chen</a>

                  </div>
                  <div class="periodical">
                    <em>In Proc. of the Association for the Advancement of Artificial Intelligence (AAAI) 2025</em>
                  </div>
                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                    <a href="https://arxiv.org/abs/2309.10230" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">PDF</a>
                    <a href="https://github.com/Daniellli/PAD.git" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Code</a>


                    <a href="https://www.youtube.com/watch?v=nvLgVpTZhu8" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Video Demo</a>
                    <a href="https://mp.weixin.qq.com/s/Or67Xr4E_HvOcV_BOKp0Qg" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">blog</a>
                      

                    <a href="https://drive.google.com/file/d/1cBOF-W2nllM12zANxAl0O6DRMz5KCea1/view?usp=drive_link" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">poster</a>
                      

                  </div>

                  <div class="abstract hidden">
                    <p>
                      LiDAR-based semantic scene understanding is an important
                      module in the modern autonomous driving perception stack.
                      However, identifying Out-Of-Distribution points in a
                      LiDAR point cloud is challenging as point clouds lack semantically rich features when compared
                      with RGB images. We
                      revisit this problem from the perspective of selective classification, which introduces a
                      selective function into the standard
                      closed-set classification setup. Our solution is built upon the
                      basic idea of abstaining from choosing any known categories
                      but learns a point-wise abstaining penalty with a marginbased loss. Synthesizing outliers to
                      approximate unlimited
                      OOD samples is also critical to this idea, so we propose a
                      strong synthesis pipeline that generates outliers originated
                      from various factors: unrealistic object categories, sampling
                      patterns and sizes. We demonstrate that learning different abstaining penalties, apart from
                      point-wise penalty, for different
                      types of (synthesized) outliers can further improve the performance. We benchmark our method on
                      SemanticKITTI and
                      nuScenes and achieve state-of-the-art results. Risk-coverage
                      analysis further reveals intrinsic properties of different methods.
                    </p>
                  </div>
                </div>
              </div>
            </li>



            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">
                    <img src="/assets/teaser/dkt-teaser.gif"
                      alt="Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation"
                      style="width: 100%;">
                  </div>

                </div>

                <div id="Schwarz2020NEURIPT" class="col-md-9">
                  <div class="title">Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation</div>

                  <div class="author">
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Songlin Wei</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Qizhe Wei</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Zheng Geng</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Hong Li</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Licheng Shen</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Qianpu Sun</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Shu Han</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Bin Ma</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Bohan Li</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Chongjie Ye</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yuhang Zheng</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Nan Wang</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Saining Zhang</a>,
                    and <a href="#" target="_blank" rel="noopener noreferrer">Hao Zhao</a>

                  </div>
                  <div class="periodical">
                    <em>In Submission, 2025</em>
                  </div>
                  <div class="links">
                    <a href="https://arxiv.org/abs/2512.23705" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">PDF</a>

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                    <a href="https://github.com/Daniellli/DKT" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Code</a>

                    <a href="https://daniellli.github.io/projects/DKT/" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Project Page</a>
                      
                    <a href="https://huggingface.co/spaces/Daniellesry/DKT" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Demo</a>
                    
                    <a href="https://huggingface.co/datasets/Daniellesry/TransPhy3D" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Dataset</a>

                    <a href="https://www.youtube.com/watch?v=Vurjdwa_y38" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">YouTube Video</a>

                    
                      
                  </div>

                  <div class="abstract hidden">
                    <p>
                      Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates.<br>
                      <br>
                      Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules.<br>
                      <br>
                      We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences (1.32M frames) rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials.<br>
                      We render RGB + depth + normals with physically based ray tracing and OptiX denoising.<br>
                      <br>
                      Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters.<br>
                      During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos.<br>
                      <br>
                      The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test.<br>
                      It improves accuracy and temporal consistency over strong image/video baselines (e.g., Depth-Anything-v2, DepthCrafter), and a normal variant (DKT-Normal) sets the best video normal estimation results on ClearPose.<br>
                      A compact 1.3B version runs at ~0.17 s/frame (832×480).<br>
                      <br>
                      Integrated into a grasping stack, DKT’s depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators.<br>
                      <br>
                      Together, these results support a broader claim: “Diffusion knows transparency.” Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.
                    </p>
                  </div>
                </div>
              </div>
            </li>




            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">
                    <img src="/assets/teaser/bilateraldriving_teaser.gif"
                      alt="Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian Splatting"
                      style="width: 100%;">
                  </div>

                </div>

                <div id="Schwarz2020NEURIPT" class="col-md-9">
                  <div class="title">Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian Splatting</div>

                  <div class="author">
                    <a href="#" target="_blank" rel="noopener noreferrer">Nan Wang</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yuantao Chen</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Lixing Xiao</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Weiqing Xiao</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Bohan Li</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Zhaoxi Chen</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Chongjie Ye</a>,
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Saining Zhang</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Ziyang Yan</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Pierre Merriaux</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Lei Lei</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Tianfan Xue</a>,
                    and <a href="#" target="_blank" rel="noopener noreferrer">Hao Zhao</a>
                  </div>
                  <div class="periodical">
                    <em>In Proc. of the Conference on Neural Information Processing Systems (NeurIPS) 2025</em>
                  </div>
                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                    <a href="https://arxiv.org/abs/2506.05280" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">PDF</a>
                    <a href="https://github.com/BigCiLeng/bilateral-driving" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Code</a>
                    <a href="https://bigcileng.github.io/bilateral-driving/" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Project Page</a>

                    
                  </div>

                  <div class="abstract hidden">
                    <p>
                      Neural rendering techniques, including NeRF and Gaussian Splatting (GS), rely on photometric consistency to produce high-quality reconstructions.<br>
                      However, in real-world scenarios, it is challenging to guarantee perfect photometric consistency in acquired images.<br>
                      Appearance codes have been widely used to address this issue, but their modeling capability is limited, as a single code is applied to the entire image.<br>
                      Recently, the bilateral grid was introduced to perform pixel-wise color mapping, but it is difficult to optimize and constrain effectively.<br>
                      In this paper, we propose a novel multi-scale bilateral grid that unifies appearance codes and bilateral grids.<br>
                      We demonstrate that this approach significantly improves geometric accuracy in dynamic, decoupled autonomous driving scene reconstruction, outperforming both appearance codes and bilateral grids.<br>
                      This is crucial for autonomous driving, where accurate geometry is important for obstacle avoidance and control.<br>
                      Our method shows strong results across four datasets: Waymo, NuScenes, Argoverse, and PandaSet.<br>
                      We further demonstrate that the improvement in geometry is driven by the multi-scale bilateral grid, which effectively reduces floaters caused by photometric inconsistency.
                    </p>
                  </div>
                </div>
              </div>
            </li>





            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">
                    <img src="/assets/teaser/oneposeviagen-teaser.gif"
                      alt="OnePoseViagen: One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation"
                      style="width: 100%;">
                  </div>

                </div>

                <div id="Schwarz2020NEURIPT" class="col-md-9">
                  <div class="title">One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</div>

                  <div class="author">
                    <a href="#" target="_blank" rel="noopener noreferrer">Zheng Geng</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Nan Wang</a>,
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Chongjie Ye</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Bohan Li</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Zhaoxi Chen</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Sida Peng</a>,
                    and <a href="#" target="_blank" rel="noopener noreferrer">Hao Zhao</a>

                  </div>
                  <div class="periodical">
                    <em>In Proc. of the Conference on Robot Learning (CoRL) 2025</em>
                  </div>
                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                    <a href="https://arxiv.org/abs/2509.07978" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">PDF</a>
                    <a href="https://github.com/gzwsama/OnePoseviaGen" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Code</a>
                    <a href="https://gzwsama.github.io/OnePoseviaGen.github.io/" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Project Page</a>

                    
                  </div>

                  <div class="abstract hidden">
                    <p>
                      Estimating the 6D pose of arbitrary unseen objects from a single reference image is critical for robotics operating in the long-tail of real-world instances.<br>
                      However, this setting is notoriously challenging: 3D models are rarely available, single-view reconstructions lack metric scale, and domain gaps between generated models and real-world images undermine robustness.<br>
                      We propose OnePoseViaGen, a pipeline that tackles these challenges through two key components.<br>
                      First, a coarse-to-fine alignment module jointly refines scale and pose by combining multi-view feature matching with render-and-compare refinement.<br>
                      Second, a text-guided generative domain randomization strategy diversifies textures, enabling effective fine-tuning of pose estimators with synthetic data.<br>
                      Together, these steps allow high-fidelity single-view 3D generation to support reliable one-shot 6D pose estimation.<br>
                      On challenging benchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves state-of-the-art performance far surpassing prior approaches.<br>
                      We further demonstrate robust dexterous grasping with a real robot hand, validating the practicality of our method in real-world manipulation.
                    </p>
                  </div>
                </div>
              </div>
            </li>
            



            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">
                    <img src="/assets/teaser/orv-teaser.gif"
                      alt="ORV: 4D Occupancy-centric Robot Video Generation"
                      style="width: 100%;">
                  </div>

                </div>

                <div id="Schwarz2020NEURIPT" class="col-md-9">
                  <div class="title">ORV: 4D Occupancy-centric Robot Video Generation</div>

                  <div class="author">
                    <a href="#" target="_blank" rel="noopener noreferrer">Xiuyu Yang*</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Bohan Li*</a>,
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Nan Wang</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Chongjie Ye</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Zhaoxi Chen</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Minghan Qin</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yikang Ding</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Xin Jin</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Hang Zhao</a>,
                    and <a href="#" target="_blank" rel="noopener noreferrer">Hao Zhao</a>

                  </div>
                  <div class="periodical">
                    <em>In Submission, 2025</em>
                  </div>
                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                    <a href="https://arxiv.org/abs/2506.03079" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">PDF</a>
                    <a href="https://github.com/OrangeSodahub/ORV" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Code</a>
                    <a href="https://orangesodahub.github.io/ORV/" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Project Page</a>

                    
                  </div>

                  <div class="abstract hidden">
                    <p>
                      Acquiring real-world robotic simulation data through teleoperation is notoriously time-consuming and labor-intensive.<br>
                      Recently, action-driven generative models have gained widespread adoption in robot learning and simulation, as they eliminate safety concerns and reduce maintenance efforts.<br>
                      However, the action sequences used in these methods often result in limited control precision and poor generalization due to their globally coarse alignment.<br>
                      To address these limitations, we propose ORV, an Occupancy-centric Robot Video generation framework, which utilizes 4D semantic occupancy sequences as a fine-grained representation to provide more accurate semantic and geometric guidance for video generation.<br>
                      By leveraging occupancy-based representations, ORV enables seamless translation of simulation data into photorealistic robot videos, while ensuring high temporal consistency and precise controllability.<br>
                      Furthermore, our framework supports the simultaneous generation of multi-view videos of robot gripping operations—an important capability for downstream robotic learning tasks.<br>
                      Extensive experimental results demonstrate that ORV consistently outperforms existing baseline methods across various datasets and sub-tasks.
                    </p>
                  </div>
                </div>
              </div>
            </li>







            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">
                    <img src="/assets/teaser/lino_teaser.png"
                      alt="Light of Normals: Unified Feature Representation for Universal Photometric Stereo"
                      style="width: 100%;">
                  </div>

                </div>

                <div id="Schwarz2020NEURIPT" class="col-md-9">
                  <div class="title">Light of Normals: Unified Feature Representation for Universal Photometric Stereo</div>

                  <div class="author">
                    <a href="#" target="_blank" rel="noopener noreferrer">Hong Li*</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Houyuan Chen*</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Chongjie Ye</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Zhaoxi Chen</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Bohan Li</a>,
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Xianda Guo</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Xuhui Liu</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yikai Wang</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Baochang Zhang</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Satoshi Ikehata</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Boxin Shi</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Anyi Rao</a>,
                    and <a href="#" target="_blank" rel="noopener noreferrer">Hao Zhao</a>

                  </div>
                  <div class="periodical">
                    <em>In Submission, 2025</em>
                  </div>
                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                    <a href="https://arxiv.org/pdf/2506.18882" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">PDF</a>
                    <a href="https://github.com/houyuanchen111/LINO_UniPS?tab=readme-ov-file" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Code</a>
                    <a href="https://houyuanchen111.github.io/lino.github.io/" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Project Page</a>

                  </div>

                  <div class="abstract hidden">
                    <p>
                      Universal photometric stereo (PS) aims to recover high-quality surface normals from objects under arbitrary lighting conditions without relying on specific illumination models.<br>
                      Despite recent advances such as SDM-UniPS and Uni MS-PS, two fundamental challenges persist:<br>
                      1) the deep coupling between varying illumination and surface normal features, where ambiguity in observed intensity makes it difficult to determine whether brightness variations stem from lighting changes or surface orientation;<br>
                      and 2) the preservation of high-frequency geometric details in complex surfaces, where intricate geometries create self-shadowing, inter-reflections, and subtle normal variations that conventional feature processing operations struggle to capture accurately.
                    </p>
                  </div>
                </div>
              </div>
            </li>



























            




            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">
                    <img src="/assets/teaser/coopdetr.gif"
                      alt="CoopDETR: A Unified Cooperative Perception Framework for 3D Detection via Object Query"
                      style="width: 100%;">
                  </div>

                </div>

                <div id="Schwarz2020NEURIPT" class="col-md-9">

                  <div class="title">CoopDETR: A Unified Cooperative Perception Framework for 3D Detection via Object
                    Query</div>


                  <div class="author">
                    <a href="https://scholar.google.com/citations?user=y_vW-jcAAAAJ&hl=zh-CN" target="_blank"
                      rel="noopener noreferrer">Zhe Wang</a>,
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Zhuang Xucai</a>,
                    <a href="https://scholar.google.com.hk/citations?user=LO8GS7sAAAAJ&hl=zh-CN" target="_blank"
                      rel="noopener noreferrer">Tongda Xu</a>,
                    <a href="https://scholar.google.com/citations?user=QOZnsYYAAAAJ&hl=zh-CN" target="_blank"
                      rel="noopener noreferrer">Yan Wang</a>,
                    <a href="https://scholar.google.com/citations?hl=en&user=BzJ_GboAAAAJ&view_op=list_works&sortby=pubdate"
                      target="_blank" rel="noopener noreferrer">Jingjing Liu</a>,
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=XGnsL5MAAAAJ&view_op=list_works&sortby=pubdate"
                      target="_blank" rel="noopener noreferrer">Yilun Chen</a>, and
                    <a href="https://scholar.google.com/citations?user=mDOMfxIAAAAJ&hl=en" target="_blank"
                      rel="noopener noreferrer">Ya-Qin Zhang</a>,
                  </div>
                  <div class="periodical">
                    <em>In Proc. of the IEEE International Conference on Robotics & Automation (ICRA) 2025</em>
                    <!-- 2024 -->
                  </div>
                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                    <a href="https://arxiv.org/abs/2502.19313" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">PDF</a>
                    <!-- <a href="https://github.com/Daniellli/PAD.git" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Code</a> -->


                    <a href="https://www.youtube.com/watch?v=JkhE2jGVV7c" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Video Demo</a>

                  </div>

                  <div class="abstract hidden">
                    <p>
                      Cooperative perception enhances the individual
                      perception capabilities of autonomous vehicles (AVs) by providing a comprehensive view of the
                      environment. However, balancing perception performance and transmission costs remains a
                      significant challenge. Current approaches that transmit regionlevel features across agents are
                      limited in interpretability and
                      demand substantial bandwidth, making them unsuitable for
                      practical applications. In this work, we propose CoopDETR, a
                      novel cooperative perception framework that introduces objectlevel feature cooperation via object
                      query. Our framework
                      consists of two key modules: single-agent query generation,
                      which efficiently encodes raw sensor data into object queries,
                      reducing transmission cost while preserving essential information for detection; and cross-agent
                      query fusion, which includes
                      Spatial Query Matching (SQM) and Object Query Aggregation
                      (OQA) to enable effective interaction between queries. Our
                      experiments on the OPV2V and V2XSet datasets demonstrate
                      that CoopDETR achieves state-of-the-art performance and
                      significantly reduces transmission costs to 1/782 of previous methods
                    </p>
                  </div>
                </div>
              </div>
            </li>




          </ol>

          <h2 class="year">2024</h2>
          <ol class="bibliography">

            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">
                    <img src="/assets/teaser/ect_teaser.gif"
                      alt="ECT: Fine-grained Edge Detection with Learned Cause Tokens" style="width: 100%;">
                  </div>

                </div>

                <div id="Schwarz2020NEURIPT" class="col-md-9">
                  <div class="title">ECT: Fine-grained Edge Detection with Learned Cause Tokens</div>

                  <div class="author">
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    <a href="https://github.com/cxx226/cxx226.github.io/blob/master/about.md" target="_blank"
                      rel="noopener noreferrer">
                      Xiaoxue Chen
                    </a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yuhang Zheng</a>,
                    <a href="https://air.tsinghua.edu.cn/info/1046/1199.htm" target="_blank"
                      rel="noopener noreferrer">Guyue Zhou</a>,
                    <a href="https://www.intel.com/content/www/us/en/research/researchers/yurong-chen.html"
                      target="_blank" rel="noopener noreferrer">Yurong Chen</a>,
                    <a href="https://www.cis.pku.edu.cn/info/1177/1379.htm" target="_blank"
                      rel="noopener noreferrer">Hongbin Zha</a>,
                    and <a href="https://sites.google.com/view/fromandto" target="_blank" rel="noopener noreferrer">Hao
                      Zhao</a>
                  </div>
                  <div class="periodical">
                    <!-- <em>In Advances in Neural Information Processing Systems (NeurIPS)</em> -->
                    <em><a href="https://www.sciencedirect.com/science/article/abs/pii/S0262885624000507" target="_blank"
                        rel="noopener noreferrer">Image and Vision Computing (IVC, JCR1, IF4.7)</a></em>
                    2024
                  </div>
                  <div class="links">
                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>

                    <a href="https://authors.elsevier.com/c/1ie6oxnVKF2-x" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">PDF</a>

                    <!-- <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS_supplementary.pdf"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a> -->

                    <!-- <a href="https://autonomousvision.github.io/graf/" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Blog</a> -->

                    <a href="https://github.com/Daniellli/ECT.git" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Code</a>

                    <!-- <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS_poster.pdf"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> -->

                    <!-- <a href="https://www.bilibili.com/video/BV1Lz4y1W7rN/?vd_source=3dbee2c7b78b0a77ef3b8f9b2ac6c209"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video Demo</a> -->
                    <a href="https://www.youtube.com/watch?v=MdtK8SKo0nc" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Video Demo</a>

                  </div>
                  <!-- Hidden abstract block -->
                  <div class="abstract hidden">
                    <p>
                      In this study, we tackle the challenging fine-grained edge detection task, which refers to
                      predicting specific edges caused by reflectance,
                      illumination, normal, and depth changes, respectively. Prior methods exploit multi-scale
                      convolutional networks,
                      which are limited in three aspects: (1) Convolutions are local operators while identifying the
                      cause of edge formation requires looking at far away pixels.
                      (2) Priors specific to edge cause are fixed in prediction heads.
                      (3) Using separate networks for generic and fine-grained edge detection, and the constraint
                      between them may be violated.
                      To address these three issues, we propose a two-stage transformer-based network sequentially
                      predicting generic edges and fine-grained edges,
                      which has a global receptive field thanks to the attention mechanism. The prior knowledge of edge
                      causes is formulated as four learnable cause tokens in
                      a cause-aware decoder design. Furthermore, to encourage the consistency between generic edges and
                      fine-grained edges,
                      an edge aggregation and alignment loss is exploited. We evaluate our method on the public
                      benchmark BSDS-RIND and several newly derived benchmarks,
                      and achieve new state-of-the-art results.
                    </p>
                  </div>
                  <!-- Hidden bibtex block -->
                </div>
              </div>
            </li>


            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">

                    <img src="/assets/teaser/iot-teaser.gif"
                      alt="Intelligent Beehive Monitoring System based on Internet of Things and Colony State Analysis"
                      style="width: 100%;">
                  </div>

                </div>


                <div id="Schwarz2020NEURIPT" class="col-md-9">
                  <div class="title">Intelligent Beehive Monitoring System based on Internet of Things and Colony State
                    Analysis</div>

                  <div class="author">
                    <a href="#" target="_blank" rel="noopener noreferrer">Yiyao Zheng</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Xiaoyan Cao</a>,
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Shihui Guo</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Rencai Huang</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yingjiao Li</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yijie Chen</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Liulin Yang</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Xiaoyu Cao</a>,
                    and <a href="#" target="_blank" rel="noopener noreferrer">Hongting Sun</a>
                  </div>
                  <div class="periodical">
                    <em>Smart Agricultural Technology (JCR1, IF6.3) </em>
                    2024
                  </div>
                  <div class="links">
                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                    <a href="https://www.sciencedirect.com/science/article/pii/S2772375524001898?via%3Dihub"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                    <!-- <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS_supplementary.pdf"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a> -->

                    <!-- <a href="https://github.com/AIR-DISCOVER/INT2.git" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Code</a> -->
                    <a href="https://www.bilibili.com/video/BV1Eo4y1k7BC/?spm_id_from=333.999.0.0&vd_source=3dbee2c7b78b0a77ef3b8f9b2ac6c209"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video
                      Demo</a>
                  </div>
                  <!-- Hidden abstract block -->
                  <div class="abstract hidden">
                    <p>
                      CONTEXT: Bees play a crucial role in terrestrial ecosystems.
                      However, Beekeepers are unable to monitor the state of beehives (bees and environment) all the
                      time,
                      which often results in Colony Collapse Disorder (CCD). Currently,
                      some researchers provided the scheme of intelligent beehive monitoring system equipped
                      with the Internet of Things (IoT). There remain two challenges:
                      Accurately monitor the environmental status around the hive and accurately track and monitor bees
                      in real time.
                      <br>

                      OBJECTIVE: With the development of the IoT and computer vision algorithms,
                      we hope to provide an automated and efficient system to meet the above challenges.
                      In this paper, we propose a hive monitoring system and build a visualization module
                      in the cloud to monitor the activity of bee colonies and the environmental dynamic changes.

                      <br>

                      METHODS: We proposed a multi-bee tracking algorithm to solve
                      the problem of monitoring bees at the door of the hive.
                      We construct a dataset containing various complex scenes, named BEE22,
                      for training and testing the performance of our algorithm.
                      We design a bee counting rule, based on results of multi-bee tracking algorithm,
                      to reasonably count the bees entering or leaving the beehive.
                      We have deployed multiple sensors around (center, margin, door, and environment)
                      the hive to accurately reflect the changes in the environment around the hive.


                      <br>
                      RESULTS AND CONCLUSIONS: Experimental results demonstrate the effectiveness and superiority
                      of our system. In particular, the tracking performance of the multi-bee tracking algorithm
                      reaches 83.5% ± 0.7% accuracy (MOTA) and 77.3% ± 0.2% MOTP, speeds up to 16 frames per second,
                      compared with other algorithms, MOTA and IDF1 are improved by 24.4% and 50.4% respectively.
                      Moreover, our counting algorithm also achieved excellent results,
                      with root mean square error (RMSE) of 0.17 ± 0.01, 1.55 ± 0.05, and 1.25 ± 0.06 in counting
                      the number of bees entering, leaving, and the current scene in an episode, respectively.
                      After that, the system will be deployed and monitored for a long time in the actual scenario,
                      it was found that the activity of bees decreased significantly under heavy rainfall conditions.
                      Additionally, the activity of the bee colony will also increase accordingly,
                      when the amplitude is 500dB to 2000dB, the temperature of the center of the beehive is 25°C to
                      37°C,
                      and the humidity is 48% to 67%.

                      <br>

                      SIGNIFICANCE: Our system can provide valuable information for bee farmers to make control
                      decisions on hives.
                    </p>
                  </div>
                  <!-- Hidden bibtex block -->
                </div>
              </div>
            </li>
          </ol>
          <h2 class="year">2023</h2>

          <ol class="bibliography">

            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">

                    <img src="/assets/teaser/int2_teaser.gif"
                      alt="INT2: Interactive Trajectory Prediction at Intersections" style="width: 100%;">
                  </div>

                </div>

                <div id="Schwarz2020NEURIPT" class="col-md-9">
                  <div class="title">INT2: Interactive Trajectory Prediction at Intersections</div>

                  <div class="author">
                    <a href="#" target="_blank" rel="noopener noreferrer">Zhijie Yan</a>,
                    <a href="https://scholar.google.com/citations?user=hmii_L8AAAAJ&hl=zh-CN&oi=sra" target="_blank"
                      rel="noopener noreferrer">Pengfei Li</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Zheng Fu</a>,
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yongliang Shi</a>,
                    <a href="https://github.com/cxx226/cxx226.github.io/blob/master/about.md" target="_blank"
                      rel="noopener noreferrer">Xiaoxue Chen</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yuhang Zheng</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Yang Li</a>,
                    <a href="https://scholar.google.com/citations?user=NAt3vgcAAAAJ&hl=zh-CN&oi=sra" target="_blank"
                      rel="noopener noreferrer">Tianyu Liu</a>,

                    <a href="#" target="_blank" rel="noopener noreferrer">Chuxuan Li</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Nairui Luo</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Xu Gao</a>,
                    <a href="https://air.tsinghua.edu.cn/en/info/1046/1621.htm" target="_blank"
                      rel="noopener noreferrer">Yilun Chen</a>,

                    <a href="http://wangzuoxu.cn/" target="_blank" rel="noopener noreferrer">Zuoxu Wang</a>,
                    <a href="https://scholar.google.com/citations?user=KlHuj2QAAAAJ&hl=zh-CN&oi=ao" target="_blank"
                      rel="noopener noreferrer">Yifeng Shi </a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Pengfei Huang</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Zhengxiao Han</a>,
                    <a href="#" target="_blank" rel="noopener noreferrer">Jirui Yuan</a>,
                    <a href="https://scholar.google.com/citations?user=AktmI14AAAAJ&hl=zh-CN&oi=ao" target="_blank"
                      rel="noopener noreferrer">Jiangtao Gong</a>,
                    <a href="https://air.tsinghua.edu.cn/info/1046/1199.htm" target="_blank"
                      rel="noopener noreferrer">Guyue Zhou</a>,
                    <a href="https://hangzhaomit.github.io/" target="_blank" rel="noopener noreferrer">Hang Zhao</a> and
                    <a href="https://sites.google.com/view/fromandto" target="_blank" rel="noopener noreferrer">Hao
                      Zhao</a>
                  </div>
                  <div class="periodical">
                    <em>In Proc. of the IEEE International Conf. on Computer Vision (ICCV)</em>
                    2023
                  </div>
                  <div class="links">
                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>

                    <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yan_INT2_Interactive_Trajectory_Prediction_at_Intersections_ICCV_2023_paper.html"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                    <!-- <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS_supplementary.pdf"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a> -->

                    <a href="https://github.com/AIR-DISCOVER/INT2.git" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Code</a>
                    <a href="https://int2.cn/" class="btn btn-sm z-depth-0" role="button" target="_blank"
                      rel="noopener noreferrer">Project Page</a>
                    <a href="https://int2.cn/download" class="btn btn-sm z-depth-0" role="button" target="_blank"
                      rel="noopener noreferrer">Dataset</a>
                    <a href="https://www.youtube.com/watch?v=KNkuakDvgVc" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">Video Demo</a>
                  </div>
                  <!-- Hidden abstract block -->
                  <div class="abstract hidden">
                    <p>
                      Motion forecasting is an important component in autonomous driving systems.
                      One of the most challenging problems in motion forecasting is interactive trajectory prediction,
                      whose goal is to jointly forecasts the future trajectories of interacting agents.
                      To this end, we present a large-scale interactive trajectory prediction dataset named INT2 for
                      INTeractive trajectory prediction at INTersections.
                      INT2 includes 612,000 scenes, each lasting 1 minute, containing up to 10,200 hours of data. The
                      agent trajectories are auto-labeled by a high-performance offline temporal detection and fusion
                      algorithm,
                      whose quality is further inspected by human judges. Vectorized semantic maps and traffic light
                      information are also provided. Additionally, the dataset poses an interesting domain mismatch
                      challenge.
                      For each intersection, we treat rush-hour and non-rush-hour segments as different domains.
                      We benchmark the best open-sourced interactive trajectory prediction method on INT2 and Waymo Open
                      Motion, under in-domain and cross-domain settings. The dataset, code and models will be made
                      public.
                    </p>
                  </div>
                  <!-- Hidden bibtex block -->
                </div>
              </div>
            </li>

          <!-- </ol>
        </div>

        <div class="publications">
          <h2>miscellaneous</h2>
          <p>
            <sup>*</sup>equal contribution; <sup>♯</sup>corresponding author.
          </p>
          <h2 class="year">2023</h2>
          <ol class="bibliography"> -->
            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">
                    <img src="/assets/teaser/ccl-paper-teaser.png"
                      alt="Foundation Models for Robotics: Best Known Practices" style="width: 100%;">
                  </div>

                </div>

                <div id="Schwarz2020NEURIPT" class="col-md-9">

                  <div class="title">Foundation Models for Robotics: Best Known Practices</div>


                  <div class="author">
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    and <a href="https://sites.google.com/view/fromandto" target="_blank" rel="noopener noreferrer">Hao
                      Zhao</a>
                  </div>
                  <div class="periodical">
                    <em>Proceedings of the 22nd Chinese National Conference on Computational Linguistics (Volume 4:
                      Tutorial Abstracts)</em>
                    2023
                  </div>
                  <div class="links">

                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                    <a href="https://aclanthology.org/2023.ccl-4.4/" class="btn btn-sm z-depth-0" role="button"
                      target="_blank" rel="noopener noreferrer">PDF</a>
                  </div>

                  <div class="abstract hidden">
                    <p>
                      Artificial general intelligence (AGI) used to be a sci-fi word but recently
                      the surprising general-ization capability of foundation models have triggered
                      a lot of attention to AGI, in both academiaand industry. Large language models
                      can now answer questions or chat with human beings,using fluent sentences and clear
                      reasoning. Diffusion models can now draw pictures of unprece-dented photo-realism,
                      according to human commands and controls. Researchers have also madesubstantial efforts
                      to explore new possibilities for robotics applications with the help of founda-tion models.
                      Since this interdisciplinary field is still under fast development,
                      there is no clearmethodological conclusions for now. In this tutorial,
                      I will briefly go through best known prac-tices that have shown transformative
                      capabilities in several sub-fields. Specifically, there are fiverepresentative paradigms:
                      (1) Using foundation models to allow human-friendly human-car in-teraction;
                      (2) Using foundation models to equip robots the capabilities of understanding vaguehuman needs;
                      (3) Using foundation models to break down complex tasks into achievable sub-tasks;
                      (4) Using foundation models to composite skill primitives so that reinforcement
                      learningcan work with sparse rewards; (5) Using foundation models to bridge languge
                      commands andlow-level control dynamics. I hope these best known practices to inspire NLP
                      researchers.
                    </p>
                  </div>
                </div>
              </div>
            </li>



            <li>
              <div class="row">
                <div class="col-md-3">
                  <div class="img-fluid rounded">
                    <img src="/assets/teaser/slip_teaser.png" alt="SLiP: Situated-oriented Localization in Point Clouds"
                      style="width: 100%;">
                  </div>

                </div>

                <div id="Schwarz2020NEURIPT" class="col-md-9">

                  <div class="title">SLiP: Situated-oriented Localization in Point Clouds</div>


                  <div class="author">
                    <strong><u><em>Shaocong Xu</em></u></strong>,
                    <a href="https://github.com/cxx226/cxx226.github.io/blob/master/about.md" target="_blank"
                      rel="noopener noreferrer"> Xiaoxue Chen</a>,
                    <a href="https://sites.google.com/view/fromandto" target="_blank" rel="noopener noreferrer">Hao
                      Zhao</a>,
                    and <a href="https://air.tsinghua.edu.cn/info/1046/1199.htm" target="_blank"
                      rel="noopener noreferrer">Guyue Zhou</a>
                  </div>
                  <div class="periodical">
                    <em>Technical Report</em>
                    2023
                  </div>
                  <div class="links">
                    <!-- <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> -->
                    <a href="https://drive.google.com/file/d/1HVc8qM0BEdVhkkOjHnTlYWoZTOy4PQAO/view?usp=drive_link"
                      class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
                  </div>

                  <!-- <div class="abstract hidden">
                    <p>
                      Artificial general intelligence (AGI) used to be a sci-fi word but recently 
                      the surprising general-ization capability of foundation models have triggered 
                      a lot of attention to AGI, in both academiaand industry. Large language models 
                      can now answer questions or chat with human beings,using fluent sentences and clear 
                      reasoning. Diffusion models can now draw pictures of unprece-dented photo-realism, 
                      according to human commands and controls. Researchers have also madesubstantial efforts 
                      to explore new possibilities for robotics applications with the help of founda-tion models. 
                      Since this interdisciplinary field is still under fast development, 
                      there is no clearmethodological conclusions for now. In this tutorial, 
                      I will briefly go through best known prac-tices that have shown transformative 
                      capabilities in several sub-fields. Specifically, there are fiverepresentative paradigms: 
                      (1) Using foundation models to allow human-friendly human-car in-teraction; 
                      (2) Using foundation models to equip robots the capabilities of understanding vaguehuman needs; 
                      (3) Using foundation models to break down complex tasks into achievable sub-tasks; 
                      (4) Using foundation models to composite skill primitives so that reinforcement 
                      learningcan work with sparse rewards; (5) Using foundation models to bridge languge 
                      commands andlow-level control dynamics. I hope these best known practices to inspire NLP researchers.
                    </p>
                  </div> -->
                </div>
              </div>
            </li>

          </ol>
        </div>
        <!--!=========================================================-->

      
        <!--!=========================Services ======================-->
        <div class="news">
          <h2>services</h2>
          <div class="table-responsive">
            <table class="table table-sm table-borderless" style="width: 100%">
              <colgroup>
                <col span="1" style="width: 18%;">
                <col span="1" style="width: 82%;">
              </colgroup>
              <tbody>
                <tr>
                  <th scope="row" style="vertical-align: top; padding-top: 0.75rem;">
                    <strong>Conference Reviewer</strong>
                  </th>
                  <td style="padding-top: 0.75rem;">
                    <ul style="margin-bottom: 0; padding-left: 1.2rem;">
                      <li style="margin-bottom: 0.4rem;">
                        Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>
                      </li>
                      <li style="margin-bottom: 0.4rem;">
                        International Conference on Learning Representations <strong>(ICLR)</strong>
                      </li>
                      <li style="margin-bottom: 0.4rem;">
                        International Conference on Intelligent Robots and Systems <strong>(IROS)</strong>
                      </li>
                      <li style="margin-bottom: 0;">
                        The AAAI Conference on Artificial Intelligence <strong>(AAAI)</strong>
                      </li>
                    </ul>
                  </td>
                </tr>
                <tr>
                  <th scope="row" style="vertical-align: top; padding-top: 0.75rem;">
                    <strong>Journal Reviewer</strong>
                  </th>
                  <td style="padding-top: 0.75rem;">
                    <ul style="margin-bottom: 0; padding-left: 1.2rem;">
                      <li style="margin-bottom: 0;">
                        IEEE Transactions on Image Processing <strong>(TIP)</strong>
                      </li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>
        <!--!=========================================================-->

                
      </article>
    </div>
  </div>

  <!-- Footer -->






  <footer class="fixed-bottom">
    <div class="container mt-0">
      © Copyright 2023 Shaocong Xu.
      Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a
        href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme.
      Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.
      Last updated: December 07, 2022.
    </div>
  </footer>



</body>

<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"
  integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg=="
  crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js"
  integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A=="
  crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"
  integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ=="
  crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js"
  integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw=="
  crossorigin="anonymous"></script>


<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>





<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"
  integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script"
  src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>




<script type="text/javascript"
  src="//rf.revolvermaps.com/0/0/8.js?i=5ukwd0bfk23&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33"
  async="async"></script>

<!-- counter  -->
<!-- <a href="https://info.flagcounter.com/o4lI"><img
    src="https://s11.flagcounter.com/count/o4lI/bg_FFFFFF/txt_000000/border_CCCCCC/columns_1/maxflags_12/viewers_3/labels_1/pageviews_1/flags_0/percent_0/"
    alt="Flag Counter" border="0"></a> -->
    
<!-- new counter  -->
<a href="https://info.flagcounter.com/WncC"><img src="https://s01.flagcounter.com/mini/WncC/bg_EECCFF/txt_000000/border_CCCCCC/flags_0/" alt="Flag Counter" border="0"></a>




</html>